TRAIN:
  # Datasets to train on
  # Available dataset list: datasets.dataset_catalog.DATASETS.keys()
  # If multiple datasets are listed, the model is trained on their union
  DATASET: 'voc_2012_trainval'
  # Scales to use during training
  # Each scale is the pixel size of an image's shortest side
  # If multiple scales are listed, then one is selected uniformly at random for
  # each training image (i.e., scale jitter data augmentation)
  SCALES: [480, 576, 688, 864, 1200]


  # Number of umages between each optmizer update.
  ITERATION_SIZE: 4

  # Images *per GPU* in the training minibatch
  # Total images per minibatch: IMS_PER_BATCH * NUM_GPUS  
  # Zeni: Legacy, I do not have two gpus ( :( )
  IMS_PER_BATCH: 2

  # RoI minibatch size *per image* (number of regions of interest [ROIs])
  # Total number of RoIs per training minibatch =
  #   BATCH_SIZE_PER_IM * IMS_PER_BATCH * NUM_GPUS
  # E.g., a common configuration is: 512 * 2 * 8: 8192
  BATCH_SIZE_PER_IM: 4096

  # Use horizontally-flipped images during training?
  USE_FLIPPED: True

  # Train using these proposals
  # During training, all proposals specified in the file are used (no limit is
  # applied)
  # Proposal files must be in correspondence with the datasets listed in
  # DATASETS
  PROPOSAL_FILES: 'data/selective_search_data/voc_2012_trainval.pkl'


  # Filter proposals that are inside of crowd regions by CROWD_FILTER_THRESH
  # "Inside" is measured as: proposal-with-crowd intersection area divided by
  # proposal area
  CROWD_FILTER_THRESH: 0

  # Ignore ground-truth objects with area < this threshold
  GT_MIN_AREA: -1

  # Freeze the backbone architecture during training if set to True
  FREEZE_CONV_BODY: False

  FG_THRESH: 0.5

  BG_THRESH: 0.1


ADAPTATIVE_SUP:
  TYPE: log
  LB: 100
  DO_TRICK: True
  ADAPTATIVE_IGN: True


# ---------------------------------------------------------------------------- #
# Data loader options
# ---------------------------------------------------------------------------- #
DATA_LOADER:
  # Number of Python threads to use for the data loader (warning: using too many
  # threads can cause GIL-based interference with Python Ops leading to *slower*
  # training; 4 seems to be the sweet spot in our experience)
  NUM_THREADS: 4



# ---------------------------------------------------------------------------- #
# Inference ('test') options
# ---------------------------------------------------------------------------- #
TEST:

  # Scale to use during testing (can NOT list multiple scales)
  # The scale is the pixel size of an image's shortest side
  # SCALE: 480

  # Max pixel size of the longest side of a scaled input image
  MAX_SIZE: 2000

  # Overlap threshold used for non-maximum suppression (suppress boxes with
  # IoU >= this threshold)
  NMS: 0.3

  # Test using these proposal files (must correspond with DATASETS)
  PROPOSAL_FILES: ['data/selective_search_data']

  # Limit on the number of proposals per image used during inference
  PROPOSAL_LIMIT: -1

  # Maximum number of detections to return per image (100 is based on the limit
  # established for the COCO dataset)
  DETECTIONS_PER_IM: 100

  # Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to
  # balance obtaining high recall with not having too many low precision
  # detections that will slow down inference post processing steps (like NMS)
  SCORE_THRESH: 0.00001

  # Save detection results files if True
  # If false, results files are cleaned up (they can be large) after local
  # evaluation
  COMPETITION_MODE: True

  # Evaluate detections with the COCO json dataset eval code even if it's not the
  # evaluation code for the dataset (e.g. evaluate PASCAL VOC results using the
  # COCO API to get COCO style AP on PASCAL VOC)
  FORCE_JSON_DATASET_EVAL: False

  # [Inferred value; do not set directly in a config]
  # Indicates if precomputed proposals are used at test time
  # Not set for 1-stage models and 2-stage models with RPN subnetwork enabled
  PRECOMPUTED_PROPOSALS: True


  # ---------------------------------------------------------------------------- #
  # Test-time augmentations for bounding box detection
  # See configs/test_time_aug/e2e_mask_rcnn_R-50-FPN_2x.yaml for an example
  # ---------------------------------------------------------------------------- #
  BBOX_AUG:
    # Enable test-time augmentation for bounding box detection if True
    ENABLED: True

    # Heuristic used to combine predicted box scores
    #   Valid options: ('ID', 'AVG', 'UNION')
    SCORE_HEUR: 'AVG'

    # Heuristic used to combine predicted box coordinates
    #   Valid options: ('ID', 'AVG', 'UNION')
    COORD_HEUR: 'ID'

    # Horizontal flip at the original scale (id transform)
    H_FLIP: True

    # Each scale is the pixel size of an image's shortest side
    SCALES: [480, 576, 688, 864, 1200]

    # Max pixel size of the longer side
    MAX_SIZE: 4000

    # Horizontal flip at each scale
    SCALE_H_FLIP: True

    # Apply scaling based on object size
    SCALE_SIZE_DEP: False
    AREA_TH_LO: 2500
    AREA_TH_HI: 32400

    # Each aspect ratio is relative to image width
    ASPECT_RATIOS: []

    # Horizontal flip at each aspect ratio
    ASPECT_RATIO_H_FLIP: False

  # ---------------------------------------------------------------------------- #
  # Soft NMS
  # ---------------------------------------------------------------------------- #
  SOFT_NMS:
    # Use soft NMS instead of standard NMS if set to True
    ENABLED: False
    # See soft NMS paper for definition of these options
    METHOD: 'linear'
    SIGMA: 0.5
    # For the soft NMS overlap threshold, we simply use NMS



# ---------------------------------------------------------------------------- #
# Model options
# ---------------------------------------------------------------------------- #
MODEL:
  # The type of model to use
  # The string must match a function in the modeling.model_builder module
  # (e.g., 'generalized_rcnn', 'mask_rcnn', ...)
  TYPE: generalized_rcnn

  # The backbone conv body to use
  CONV_BODY: vgg16.dilated_conv5_body

  # Number of classes in the dataset; must be set
  # E.g., 81 for COCO (80 foreground + 1 background)
  NUM_CLASSES: 20

  # Whether to load imagenet pretrained weights
  # If True, path to the weight file must be specified.
  # See: RESNETS.IMAGENET_PRETRAINED_WEIGHTS
  LOAD_IMAGENET_PRETRAINED_WEIGHTS: True



# ---------------------------------------------------------------------------- #
# Solver options
# Note: all solver options are used exactly as specified; the implication is
# that if you switch from training on 1 GPU to N GPUs, you MUST adjust the
# solver configuration accordingly. We suggest using gradual warmup and the
# linear learning rate scaling rule as described in
# "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" Goyal et al.
# https://arxiv.org/abs/1706.02677
# ---------------------------------------------------------------------------- #
SOLVER:

  # e.g 'SGD', 'Adam'
  TYPE: 'SGD'

  # Base learning rate for the specified schedule
  BASE_LR: 0.001

  # Schedule type (see functions in utils.lr_policy for options)
  # E.g., 'step', 'steps_with_decay', ...
  LR_POLICY: steps_with_decay

  # Some LR Policies (by example):
  # 'step'
  #   lr: BASE_LR * GAMMA ** (cur_iter // STEP_SIZE)
  # 'steps_with_decay'
  #   STEPS: [0, 60000, 80000]
  #   GAMMA: 0.1
  #   lr: BASE_LR * GAMMA ** current_step
  #   iters [0, 59999] are in current_step: 0, iters [60000, 79999] are in
  #   current_step: 1, and so on
  # 'steps_with_lrs'
  #   STEPS: [0, 60000, 80000]
  #   LRS: [0.02, 0.002, 0.0002]
  #   lr: LRS[current_step]

  # Hyperparameter used by the specified policy
  # For 'step', the current LR is multiplied by GAMMA at each step
  GAMMA: 0.1

  # Uniform step size for 'steps' policy
  STEP_SIZE: 30000

  # Non-uniform step iterations for 'steps_with_decay' or 'steps_with_lrs'
  # policies
  STEPS: [0, 75000]

  # Learning rates to use with 'steps_with_lrs' policy
  LRS: []

  # Maximum number of SGD iterations
  MAX_ITER: 90000
  

  # Momentum to use with SGD
  MOMENTUM: 0.9

  # L2 regularization hyperparameter
  WEIGHT_DECAY: 0.0005
  # L2 regularization hyperparameter for GroupNorm's parameters
  WEIGHT_DECAY_GN: 0.0

  # Whether to double the learning rate for bias
  BIAS_DOUBLE_LR: True

  # Whether to have weight decay on bias as well
  BIAS_WEIGHT_DECAY: False

  # Warm up to BASE_LR over this number of SGD iterations
  WARM_UP_ITERS: 500

  # Start the warm up from BASE_LR * WARM_UP_FACTOR
  WARM_UP_FACTOR: 0.33333333333

  # WARM_UP_METHOD can be either 'constant' or 'linear' (i.e., gradual)
  WARM_UP_METHOD: 'linear'

  # Scale the momentum update history by new_lr / old_lr when updating the
  # learning rate (this is correct given MomentumSGDUpdateOp)
  SCALE_MOMENTUM: True
  # Only apply the correction if the relative LR change exceeds this threshold
  # (prevents ever change in linear warm up from scaling the momentum by a tiny
  # amount; momentum scaling is only important if the LR change is large)
  SCALE_MOMENTUM_THRESHOLD: 1.1

  # Suppress logging of changes to LR unless the relative change exceeds this
  # threshold (prevents linear warm up from spamming the training log)
  LOG_LR_CHANGE_THRESHOLD: 1.1


# ---------------------------------------------------------------------------- #
# Fast R-CNN options
# ---------------------------------------------------------------------------- #

FAST_RCNN:

  # The type of RoI head to use for bounding box classification and regression
  # The string must match a function this is imported in modeling.model_builder
  # (e.g., 'head_builder.add_roi_2mlp_head' to specify a two hidden layer MLP)
  ROI_BOX_HEAD: vgg16.roi_2mlp_head
  MIN_PROPOSAL_SIZE: 2
  TOP_K: -1
  # Hidden layer dimension when using an MLP for the RoI box head
  MLP_HEAD_DIM: 4096

  # Hidden Conv layer dimension when using Convs for the RoI box head
  CONV_HEAD_DIM: 256
  # Number of stacked Conv layers in the RoI box head
  NUM_STACKED_CONVS: 4

  # RoI transformation function (e.g., RoIPool or RoIAlign)
  # (RoIPoolF is the same as RoIPool; ignore the trailing 'F')
  ROI_XFORM_METHOD: RoIPoolF

  # Number of grid sampling points in RoIAlign (usually use 2)
  # Only applies to RoIAlign
  ROI_XFORM_SAMPLING_RATIO: 0

  # RoI transform output resolution
  # Note: some models may have constraints on what they can use, e.g. they use
  # pretrained FC layers like in VGG16, and will ignore this option
  ROI_XFORM_RESOLUTION: 7



# ---------------------------------------------------------------------------- #
# ResNets options ("ResNets": ResNet and ResNeXt)
# ---------------------------------------------------------------------------- #
VGG:

  # Freeze model weights before and including which block.
  # Choices: [0, 2, 3, 4, 5]. O means not fixed. First conv and bn are defaults to
  # be fixed.
  FREEZE_AT: 2

  # Path to pretrained resnet weights on ImageNet.
  # If start with '/', then it is treated as a absolute path.
  # Otherwise, treat as a relative path to ROOT_DIR
  IMAGENET_PRETRAINED_WEIGHTS: 'data/pretrained_model/vgg16_caffe.pth'



# ---------------------------------------------------------------------------- #
# MISC options
# ---------------------------------------------------------------------------- #

# Numer of refinement times
REFINE_TIMES: 3

# Number of GPUs to use (applies to both training and testing)
NUM_GPUS: 1

# The mapping from image coordinates to feature map coordinates might cause
# some boxes that are distinct in image space to become identical in feature
# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor
# for identifying duplicate boxes.
# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16
DEDUP_BOXES: 0.125

  # Clip bounding box transformation predictions to prevent np.exp from
  # overflowing
  # Heuristic choice based on that would scale a 16 pixel anchor up to 1000 pixels
# BBOX_XFORM_CLIP: -0.90308998699

  # Pixel mean values (BGR order) as a (1, 1, 3) array
  # We use the same pixel mean for all networks even though it's not exactly what
  # they were trained with
  # "Fun" fact: the history of where these values comes from is lost (From Detectron lol)
# PIXEL_MEANS: np.array([[[102.9801, 115.9465, 122.7717]]])

# For reproducibility
RNG_SEED: 3

# A small number that's used many times
EPS: 1e-14

# Root directory of project
# ROOT_DIR: osp.abspath(osp.join(osp.dirname(__file__), '..', '..'))

# Output basedir
OUTPUT_DIR: 'Outputs'

# Name (or path to) the matlab executable
MATLAB: 'octave'  # a

CUDA: True
DEBUG: False
START_STEP: 0